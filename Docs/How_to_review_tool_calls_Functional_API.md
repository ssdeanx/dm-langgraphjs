How to review tool calls (Functional API)

Skip to content

Our Building Ambient Agents with LangGraph course is now available on LangChain Academy!

# How to review tool calls (Functional API)¶

Prerequisites

This guide assumes familiarity with the following:

* Implementing human-in-the-loop workflows with interrupt
* How to create a ReAct agent using the Functional API

This guide demonstrates how to implement human-in-the-loop workflows in a ReAct agent using the LangGraph Functional API.

We will build off of the agent created in the How to create a ReAct agent using the Functional API guide.

Specifically, we will demonstrate how to review tool calls generated by a chat model prior to their execution. This can be accomplished through use of the interrupt function at key points in our application.

Preview:

We will implement a simple function that reviews tool calls generated from our chat model and call it from inside our application's entrypoint:

```
function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage { // Interrupt for human review const humanReview = interrupt({ question: "Is this correct?", tool_call: toolCall, }); const { action, data } = humanReview; if (action === "continue") { return toolCall; } else if (action === "update") { return { ...toolCall, args: data, }; } else if (action === "feedback") { return new ToolMessage({ content: data, name: toolCall.name, tool_call_id: toolCall.id, }); } throw new Error(`Unsupported review action: ${action}`); }
```

## Setup¶

Note

This guide requires @langchain/langgraph>=0.2.42.

First, install the required dependencies for this example:

```
npm install @langchain/langgraph @langchain/openai @langchain/core zod
```

Next, we need to set API keys for OpenAI (the LLM we will use):

```
process.env.OPENAI_API_KEY = "YOUR_API_KEY";
```

Set up LangSmith for LangGraph development

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here

## Define model and tools¶

Let's first define the tools and model we will use for our example. As in the ReAct agent guide, we will use a single place-holder tool that gets a description of the weather for a location.

We will use an OpenAI chat model for this example, but any model supporting tool-calling will suffice.

```
import { ChatOpenAI } from "@langchain/openai"; import { tool } from "@langchain/core/tools"; import { z } from "zod"; const model = new ChatOpenAI({ model: "gpt-4o-mini", }); const getWeather = tool(async ({ location }) => { // This is a placeholder for the actual implementation const lowercaseLocation = location.toLowerCase(); if (lowercaseLocation.includes("sf") || lowercaseLocation.includes("san francisco")) { return "It's sunny!"; } else if (lowercaseLocation.includes("boston")) { return "It's rainy!"; } else { return `I am not sure what the weather is in ${location}`; } }, { name: "getWeather", schema: z.object({ location: z.string().describe("Location to get the weather for"), }), description: "Call to get the weather from a specific location.", }); const tools = [getWeather];
```

## Define tasks¶

Our tasks are unchanged from the ReAct agent guide:

1. Call model: We want to query our chat model with a list of messages.
2. Call tool: If our model generates tool calls, we want to execute them.

```
import { type BaseMessageLike, AIMessage, ToolMessage, } from "@langchain/core/messages"; import { type ToolCall } from "@langchain/core/messages/tool"; import { task } from "@langchain/langgraph"; const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool])); const callModel = task("callModel", async (messages: BaseMessageLike[]) => { const response = await model.bindTools(tools).invoke(messages); return response; }); const callTool = task( "callTool", async (toolCall: ToolCall): Promise<AIMessage> => { const tool = toolsByName[toolCall.name]; const observation = await tool.invoke(toolCall.args); return new ToolMessage({ content: observation, tool_call_id: toolCall.id }); // Can also pass toolCall directly into the tool to return a ToolMessage // return tool.invoke(toolCall); });
```

## Define entrypoint¶

To review tool calls before execution, we add a reviewToolCalls function that calls interrupt. When this function is called, execution will be paused until we issue a command to resume it.

Given a tool call, our function will interrupt for human review. At that point we can either:

* Accept the tool call;
* Revise the tool call and continue;
* Generate a custom tool message (e.g., instructing the model to re-format its tool call).

We will demonstrate these three cases in the usage examples below.

```
import { interrupt } from "@langchain/langgraph"; function reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage { // Interrupt for human review const humanReview = interrupt(

<error>Content truncated. Call the fetch tool with a start_index of 5000 to get more content.</error>